{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "733a6a42"
      },
      "source": [
        "## Module Imports\n",
        "\n",
        "This cell imports all necessary PyTorch modules and other standard libraries required for building and training the HormoneLLM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1uieL4ECVtnC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19b278f7"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "This class implements positional encoding, which adds information about the position of tokens in a sequence to their embeddings. This is crucial for transformer models as they do not inherently process sequence order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=4096):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, : x.size(1)]"
      ],
      "metadata": {
        "id": "cFizv949Y6Si"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65aa7765"
      },
      "source": [
        "## Feed-Forward Network\n",
        "\n",
        "This class defines a simple feed-forward network with a GELU activation and dropout, used within the Transformer blocks for non-linear transformations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "kc0SPF75Y7nr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8bee79b"
      },
      "source": [
        "## Encoder Block\n",
        "\n",
        "This class represents a single encoder block of the Transformer architecture. It consists of a multi-head self-attention layer followed by a feed-forward network, with layer normalization and residual connections."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_out, _ = self.attn(x, x, x, key_padding_mask=mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Avb31OkAY9NV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83a8d158"
      },
      "source": [
        "## Decoder Block\n",
        "\n",
        "This class represents a single decoder block. It includes a multi-head self-attention layer, a multi-head cross-attention layer (attending to the encoder output), and a feed-forward network, all with layer normalization and residual connections."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
        "        self_attn_out, _ = self.self_attn(x, x, x, attn_mask=tgt_mask)\n",
        "        x = self.norm1(x + self_attn_out)\n",
        "\n",
        "        cross_attn_out, _ = self.cross_attn(\n",
        "            x, memory, memory, key_padding_mask=memory_mask\n",
        "        )\n",
        "        x = self.norm2(x + cross_attn_out)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm3(x + ffn_out)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PCdJXYA1Y-g-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89d287d7"
      },
      "source": [
        "## Hormone Attention Head\n",
        "\n",
        "This custom attention head is designed to extract specific 'hormone' signals from the encoder's states. It uses a learnable query to attend to the encoder outputs and then processes the attention output through an MLP to derive a scalar hormone value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HormoneAttentionHead(nn.Module):\n",
        "    def __init__(self, d_model, temperature=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.temperature = temperature\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            d_model, 1, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.query = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, encoder_states):\n",
        "        B = encoder_states.size(0)\n",
        "\n",
        "        q = self.query.expand(B, -1, -1) / self.temperature\n",
        "        attn_out, _ = self.attn(q, encoder_states, encoder_states)\n",
        "\n",
        "        return self.mlp(attn_out.squeeze(1))"
      ],
      "metadata": {
        "id": "8pIfO2mHY_5H"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8881761d"
      },
      "source": [
        "## Hormone Emotion Block\n",
        "\n",
        "This block modulates the encoder states based on the predicted hormone vector. It projects the hormone values into the model's dimension and applies a weighted scaling to the encoder states, introducing an 'emotional' influence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HormoneEmotionBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_hormones):\n",
        "        super().__init__()\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Linear(num_hormones, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.3))\n",
        "\n",
        "    def forward(self, encoder_states, hormone_vector):\n",
        "        e = self.project(hormone_vector)  # [B, D]\n",
        "\n",
        "        alpha = torch.clamp(self.alpha, 0.1, 0.5)\n",
        "        modulation = 1 + alpha * e.unsqueeze(1)\n",
        "\n",
        "        return encoder_states * modulation"
      ],
      "metadata": {
        "id": "S2ZDTG4PZBVe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4a2d961"
      },
      "source": [
        "## HormoneLLM Model Definition\n",
        "\n",
        "This is the main model class, integrating all the previously defined components. It's a Transformer-based Encoder-Decoder model with an added 'Hormone' mechanism that predicts and uses hormone-like signals to modulate the encoder's output before passing it to the decoder. It also includes an orthogonal initialization for hormone queries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HormoneLLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        hormone_names,\n",
        "        d_model=512,\n",
        "        n_heads=8,\n",
        "        d_ff=2048,\n",
        "        num_layers=6,\n",
        "        max_len=4096\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hormone_names = hormone_names\n",
        "        self.num_hormones = len(hormone_names)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderBlock(d_model, n_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Hormone Heads (dynamic)\n",
        "        self.hormone_heads = nn.ModuleDict({\n",
        "            name: HormoneAttentionHead(d_model)\n",
        "            for name in hormone_names\n",
        "        })\n",
        "\n",
        "        self.hormone_block = HormoneEmotionBlock(\n",
        "            d_model, self.num_hormones\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self._init_orthogonal_hormones()\n",
        "\n",
        "    def _init_orthogonal_hormones(self):\n",
        "        with torch.no_grad():\n",
        "            Q = torch.stack([\n",
        "                head.query.squeeze()\n",
        "                for head in self.hormone_heads.values()\n",
        "            ])  # [H, D]\n",
        "\n",
        "            Q, _ = torch.linalg.qr(Q.T)\n",
        "            Q = Q.T\n",
        "\n",
        "            for i, head in enumerate(self.hormone_heads.values()):\n",
        "                head.query.copy_(Q[i].unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "    def encode(self, input_ids, src_mask=None):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = self.positional(x)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def decode(self, tgt_ids, memory, tgt_mask=None, memory_mask=None):\n",
        "        x = self.embedding(tgt_ids)\n",
        "        x = self.positional(x)\n",
        "\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, memory, tgt_mask, memory_mask)\n",
        "\n",
        "        return self.output_head(x)\n",
        "\n",
        "    def forward(self, input_ids, decoder_ids):\n",
        "        encoder_states = self.encode(input_ids)\n",
        "\n",
        "        hormone_values = []\n",
        "        hormone_dict = {}\n",
        "\n",
        "        for name, head in self.hormone_heads.items():\n",
        "            value = head(encoder_states)\n",
        "            hormone_values.append(value)\n",
        "            hormone_dict[name] = value\n",
        "\n",
        "        hormone_vector = torch.cat(hormone_values, dim=1)\n",
        "\n",
        "        encoder_states = self.hormone_block(\n",
        "            encoder_states, hormone_vector\n",
        "        )\n",
        "\n",
        "        logits = self.decode(decoder_ids, encoder_states)\n",
        "\n",
        "        return {\n",
        "            \"logits\": logits,\n",
        "            \"hormones\": hormone_vector,\n",
        "            \"hormone_map\": hormone_dict\n",
        "        }"
      ],
      "metadata": {
        "id": "3f_zK_hVZC7Y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9af8a827"
      },
      "source": [
        "## Model and Hormone Initialization\n",
        "\n",
        "This cell defines the list of 'hormone' names to be used by the model and then initializes an instance of the `HormoneLLM` with a specified vocabulary size and model parameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hormones = [\n",
        "    \"dopamine\",\n",
        "    \"serotonin\",\n",
        "    \"cortisol\",\n",
        "    \"oxytocin\",\n",
        "    \"adrenaline\",\n",
        "    \"endorphin\",\n",
        "]\n",
        "\n",
        "model = HormoneLLM(\n",
        "    vocab_size=32000,\n",
        "    hormone_names=hormones\n",
        ")"
      ],
      "metadata": {
        "id": "TKGf1fTgZExJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7e1f457"
      },
      "source": [
        "## Simple Tokenizer\n",
        "\n",
        "This class implements a basic tokenizer for converting text to numerical IDs and vice-versa. It handles building a vocabulary from text data, encoding sentences into token IDs, and decoding token IDs back into sentences. It also supports saving and loading its vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab=None):\n",
        "        if vocab is None:\n",
        "            self.vocab = {\"<pad>\":0, \"<bos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.inv_vocab = {v:k for k,v in self.vocab.items()}\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        idx = len(self.vocab)\n",
        "        for text in texts:\n",
        "            for tok in text.lower().split():\n",
        "                if tok not in self.vocab:\n",
        "                    self.vocab[tok] = idx\n",
        "                    idx += 1\n",
        "        self.inv_vocab = {v:k for k,v in self.vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        ids = [self.vocab.get(t, self.vocab[\"<unk>\"]) for t in tokens]\n",
        "        return [self.vocab[\"<bos>\"]] + ids + [self.vocab[\"<eos>\"]]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        tokens = []\n",
        "        for i in ids:\n",
        "            if i == self.vocab[\"<eos>\"]:\n",
        "                break\n",
        "            tokens.append(self.inv_vocab.get(i, \"<unk>\"))\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(self.vocab, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        with open(path) as f:\n",
        "            vocab = json.load(f)\n",
        "        return cls(vocab)"
      ],
      "metadata": {
        "id": "Ft61sChHZGmR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dea14aa"
      },
      "source": [
        "## Toy Dataset\n",
        "\n",
        "This cell defines a small, synthetic dataset (`toy_data`) for training and demonstration purposes. Each entry includes an input text, an expected output text, and a corresponding vector of 'ground truth' hormone values. The `hormone_names` list is also defined here for clarity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hormone_names = hormones\n",
        "\n",
        "toy_data = [\n",
        "    {\n",
        "        \"input\": \"i am very happy today\",\n",
        "        \"output\": \"that is wonderful to hear\",\n",
        "        \"hormones\": [0.9, 0.8, 0.1, 0.6, 0.3, 0.7]\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"i feel sad and lonely\",\n",
        "        \"output\": \"i am here for you\",\n",
        "        \"hormones\": [0.2, 0.3, 0.6, 0.8, 0.2, 0.3]\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"i am angry right now\",\n",
        "        \"output\": \"let us calm down together\",\n",
        "        \"hormones\": [0.3, 0.2, 0.8, 0.1, 0.7, 0.2]\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"thank you for helping me\",\n",
        "        \"output\": \"you are very welcome\",\n",
        "        \"hormones\": [0.7, 0.7, 0.1, 0.9, 0.2, 0.8]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "OQRD4TLOZOFq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3f47dc1"
      },
      "source": [
        "## Tokenizer Building and Batch Creation\n",
        "\n",
        "This cell prepares the tokenizer by building its vocabulary from the `toy_data`. It also defines a utility function `make_batch` to convert a sample from the `toy_data` into PyTorch tensors suitable for model input."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "for d in toy_data:\n",
        "    texts.append(d[\"input\"])\n",
        "    texts.append(d[\"output\"])\n",
        "\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.build_vocab(texts)\n",
        "\n",
        "def make_batch(sample):\n",
        "    src = torch.tensor(tokenizer.encode(sample[\"input\"]))\n",
        "    tgt = torch.tensor(tokenizer.encode(sample[\"output\"]))\n",
        "    hormones = torch.tensor(sample[\"hormones\"], dtype=torch.float)\n",
        "    return src, tgt, hormones"
      ],
      "metadata": {
        "id": "dpnQ_GKDZTO2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5919ad65"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "This cell defines the two loss functions used for training the model:\n",
        "- `language_loss`: A cross-entropy loss for the language modeling part (predicting the next token).\n",
        "- `hormone_loss`: A Mean Squared Error (MSE) loss to guide the hormone predictions to match the target hormone values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def language_loss(logits, targets):\n",
        "    return F.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        targets.view(-1),\n",
        "        ignore_index=0\n",
        "    )\n",
        "\n",
        "def hormone_loss(pred, target):\n",
        "    return F.mse_loss(pred, target)"
      ],
      "metadata": {
        "id": "7_-m-DTuZXH5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff502168"
      },
      "source": [
        "## Model Training Loop\n",
        "\n",
        "This cell sets up and executes the training process for the `HormoneLLM`. It initializes the model, optimizer, and runs for a specified number of epochs. During each epoch, it iterates through the `toy_data`, calculates both language and hormone losses, and updates the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = HormoneLLM(\n",
        "    vocab_size=len(tokenizer.vocab),\n",
        "    hormone_names=hormone_names,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    d_ff=512,\n",
        "    num_layers=3\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "epochs = 300\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for sample in toy_data:\n",
        "        src, tgt, hormone_target = make_batch(sample)\n",
        "        src = src.unsqueeze(0).to(device)\n",
        "        tgt = tgt.unsqueeze(0).to(device)\n",
        "        hormone_target = hormone_target.unsqueeze(0).to(device)\n",
        "\n",
        "        out = model(src, tgt[:, :-1])\n",
        "\n",
        "        logits = out[\"logits\"]\n",
        "        hormone_pred = out[\"hormones\"]\n",
        "\n",
        "        l_lang = language_loss(logits, tgt[:, 1:])\n",
        "        l_horm = hormone_loss(hormone_pred, hormone_target)\n",
        "\n",
        "        loss = l_lang + 0.5 * l_horm\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7OF8DkEZZbH",
        "outputId": "3a9b08ce-197a-4fef-c29b-8ddaf090d4c2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss 14.5373\n",
            "Epoch 50 | Loss 0.1150\n",
            "Epoch 100 | Loss 0.0514\n",
            "Epoch 150 | Loss 0.0328\n",
            "Epoch 200 | Loss 0.0214\n",
            "Epoch 250 | Loss 0.0153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4144d3fb"
      },
      "source": [
        "## Chat Inference Function\n",
        "\n",
        "This function demonstrates how to use the trained `HormoneLLM` for generating responses in a chat-like manner. Given an input text, it encodes it, then iteratively decodes a response token by token until an end-of-sequence token is generated or `max_len` is reached. It also returns the predicted hormone values for the input."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model, tokenizer, text, max_len=20):\n",
        "    model.eval()\n",
        "\n",
        "    src = torch.tensor(tokenizer.encode(text)).unsqueeze(0).to(device)\n",
        "    decoder = torch.tensor([[tokenizer.vocab[\"<bos>\"]]]).to(device)\n",
        "\n",
        "    hormones_out = None\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        out = model(src, decoder)\n",
        "        logits = out[\"logits\"][:, -1]\n",
        "        next_id = logits.argmax(-1).unsqueeze(1)\n",
        "\n",
        "        decoder = torch.cat([decoder, next_id], dim=1)\n",
        "        hormones_out = out[\"hormone_map\"]\n",
        "\n",
        "        if next_id.item() == tokenizer.vocab[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    response = tokenizer.decode(decoder[0].tolist())\n",
        "    hormone_values = {\n",
        "        k: float(v.item()) for k,v in hormones_out.items()\n",
        "    }\n",
        "\n",
        "    return response, hormone_values\n"
      ],
      "metadata": {
        "id": "umvE_EUrZbnu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e043b41"
      },
      "source": [
        "## Example Chat Interaction\n",
        "\n",
        "This cell showcases an example usage of the `chat` function with the trained model and tokenizer. It provides an input phrase and prints the generated response along with the corresponding predicted hormone levels."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp, hormones = chat(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    \"i am very happy today\"\n",
        ")\n",
        "\n",
        "print(\"Response:\", resp)\n",
        "print(\"Hormones:\")\n",
        "for k,v in hormones.items():\n",
        "    print(f\"  {k}: {v:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sqHIyZ8ZlQw",
        "outputId": "c9d5a173-ed15-4946-9ee2-31eacdc852ae"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: <bos> that is wonderful to hear\n",
            "Hormones:\n",
            "  dopamine: 0.90\n",
            "  serotonin: 0.80\n",
            "  cortisol: 0.10\n",
            "  oxytocin: 0.60\n",
            "  adrenaline: 0.30\n",
            "  endorphin: 0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c0e0de"
      },
      "source": [
        "## Saving Model and Tokenizer\n",
        "\n",
        "This cell saves the trained model's state dictionary and the tokenizer's vocabulary to disk. This allows for persistence of the trained components, so they can be loaded and reused later without retraining."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"hormone_llm.pt\")\n",
        "tokenizer.save(\"tokenizer.json\")"
      ],
      "metadata": {
        "id": "dLsjb8qpZnxs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef1af054"
      },
      "source": [
        "## Loading and Testing the Model\n",
        "\n",
        "This cell demonstrates how to load a previously saved tokenizer and model. It then performs another chat interaction with the loaded model to verify that it is functioning correctly after being reloaded from disk."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_tokenizer = SimpleTokenizer.load(\"tokenizer.json\")\n",
        "\n",
        "loaded_model = HormoneLLM(\n",
        "    vocab_size=len(loaded_tokenizer.vocab),\n",
        "    hormone_names=hormone_names,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    d_ff=512,\n",
        "    num_layers=3\n",
        ").to(device)\n",
        "\n",
        "loaded_model.load_state_dict(\n",
        "    torch.load(\"hormone_llm.pt\", map_location=device)\n",
        ")\n",
        "\n",
        "resp, hormones = chat(\n",
        "    loaded_model,\n",
        "    loaded_tokenizer,\n",
        "    \"i hate you\"\n",
        ")\n",
        "\n",
        "print(\"Loaded model response:\", resp)\n",
        "print(\"Hormones:\", hormones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQQr12xbZsDy",
        "outputId": "1ebb265a-4a2b-4a34-ca3c-4d7637f42720"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model response: <bos> you\n",
            "Hormones: {'dopamine': 0.45865148305892944, 'serotonin': 0.4118742346763611, 'cortisol': 0.433440625667572, 'oxytocin': 0.5855403542518616, 'adrenaline': 0.275560587644577, 'endorphin': 0.5346636772155762}\n"
          ]
        }
      ]
    }
  ]
}